{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ud4YrxkS4uM"
      },
      "source": [
        "# Score-Based Diffusion Models\n",
        "Exercise by [Jes Frellsen](https://frellsen.org) (Technical University of Denmark), June 2025 (version 1.0).\n",
        "\n",
        "In this programming exercise, you will work with Score-Based Diffusion Models as described by [Song et al., 2021](https://arxiv.org/abs/2011.13456). We consider the MNIST dataset, with pixel values transformed the interval $[-1,1]$.\n",
        "\n",
        "The provided code is a modular and simple implementation of most of the functionality of a variance preserving diffusion model. Your task is to implement the learning and sampling functionality, as described in **the exercise falling the code**.\n",
        "\n",
        "We have provided you with one file:\n",
        "* `unet.py` contains the code for a U-Net predicting $\\epsilon$ of reverse process on MNIST. The architecture and the implementation of the U-Net is adapted from an implementation by\n",
        "[Muhammad Firmansyah Kasim](https://github.com/mfkasim1/score-based-tutorial/blob/main/03-SGM-with-SDE-MNIST.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsBYylB5S4uT"
      },
      "source": [
        "You can download the files using the following commands:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQJidO4uS4uU"
      },
      "outputs": [],
      "source": [
        "! curl -O https://raw.githubusercontent.com/frellsen/ProbAI-2025/refs/heads/main/unet.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFuUXg2yS4uX"
      },
      "source": [
        "# Implementing the diffusion model\n",
        "**Implementation:** Below we provide an implementation of a variance preserving diffusion model. The code is missing the implementation of the loss and sample function. Your task will be to complete them (see Exercise 1 below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jh78KkliS4uY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributions as td\n",
        "import math\n",
        "\n",
        "class Diffusion(nn.Module):\n",
        "    def __init__(self, network, beta_min=0.1, beta_max=20., tau=1e-3):\n",
        "        \"\"\"\n",
        "        Initialize a variance preserving score-based diffusion model.\n",
        "\n",
        "        Parameters:\n",
        "        network: [nn.Module]\n",
        "            The network to use for the diffusion process.\n",
        "        beta_min: [float]\n",
        "            The minimal beta for linear beta noise schedule.\n",
        "        beta_max: [float]\n",
        "            The maximal beta for linear beta noise schedule.\n",
        "        tau: [float]\n",
        "            The time interval goes from [tau, 1] for the diffusion process.\n",
        "        \"\"\"\n",
        "        super(Diffusion, self).__init__()\n",
        "        self.network = network\n",
        "        self.beta_min = beta_min\n",
        "        self.beta_max = beta_max\n",
        "        self.tau = tau\n",
        "\n",
        "    def beta(self, t):\n",
        "        \"\"\"\n",
        "        Compute the beta noise schedule.\n",
        "\n",
        "        Parameters:\n",
        "        t: [torch.Tensor]\n",
        "            The time steps for which to compute the beta values.\n",
        "        \"\"\"\n",
        "        return self.beta_min + t * (self.beta_max - self.beta_min)\n",
        "\n",
        "    def mu_sigma(self, t, x_0=None):\n",
        "        \"\"\"\n",
        "        Compute the mean and standard deviation for the variance preserving transistion kernel.\n",
        "\n",
        "        Parameters:\n",
        "        t: [torch.Tensor]\n",
        "            The time steps for which to compute the mean and standard deviation.\n",
        "        x_0: [torch.Tensor, optional]\n",
        "            The original data (x_0) to compute the mean. If None, only the standard deviation is computed.\n",
        "        \"\"\"\n",
        "\n",
        "        c = -0.5 * t**2 * (self.beta_max-self.beta_min) - t*self.beta_min\n",
        "        sigma = torch.sqrt(1-torch.exp(c))\n",
        "\n",
        "        if x_0 is None:\n",
        "            return sigma\n",
        "        else:\n",
        "            mu = torch.exp(0.5*c) * x_0\n",
        "            return mu, sigma\n",
        "\n",
        "    def loss(self, x_0):\n",
        "        \"\"\"\n",
        "        Evaluate the denoising score matching loss.\n",
        "\n",
        "        Parameters:\n",
        "        x_0: [torch.Tensor]\n",
        "            A batch of data (x) of dimension `(batch_size, *)`.\n",
        "        Returns:\n",
        "        [torch.Tensor]\n",
        "            The computed loss value.\n",
        "        \"\"\"\n",
        "\n",
        "        # Sample the time steps\n",
        "        t = td.uniform.Uniform(self.tau, 1).sample((x_0.shape[0],) + (x_0.dim()-1)*(1,)).to(x_0.device)\n",
        "\n",
        "        # Sample the noise\n",
        "        epsilon = torch.randn_like(x_0)\n",
        "\n",
        "        # Sample x_t from the noising process\n",
        "        mu, sigma = self.mu_sigma(t, x_0)\n",
        "        x_t = mu + sigma * epsilon\n",
        "\n",
        "        ### You code here\n",
        "        loss = 0\n",
        "        ###\n",
        "\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "    def sample(self, shape, T):\n",
        "        \"\"\"\n",
        "        Sample from the model using Euler Maruyama method for SDEs.\n",
        "\n",
        "        Parameters:\n",
        "        shape: [tuple]\n",
        "            The shape of the samples to generate.\n",
        "        T: [int]\n",
        "            The number of time steps to sample.\n",
        "        Returns:\n",
        "        [torch.Tensor]\n",
        "            The generated samples.\n",
        "        \"\"\"\n",
        "\n",
        "        device = next(self.network.parameters()).device\n",
        "\n",
        "        delta_t = (1-self.tau)/T\n",
        "\n",
        "        # Sample x_t for i=T (i.e., Gaussian noise)\n",
        "        x_t = torch.randn(shape).to(device)\n",
        "\n",
        "        # Sample x_t given x_{t+1}the time steps\n",
        "        for i in range(T, 1, -1):\n",
        "            t = self.tau + i*delta_t\n",
        "            t = torch.full((x_t.shape[0],1), t).to(device)\n",
        "\n",
        "            beta = self.beta(t)\n",
        "            sigma = self.mu_sigma(t)\n",
        "\n",
        "            delta_w = math.sqrt(delta_t)*torch.randn_like(x_t).to(device)\n",
        "\n",
        "            ### You code here\n",
        "            x_t = 0\n",
        "            ###\n",
        "\n",
        "        return x_t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftc9G2RNS4ua"
      },
      "source": [
        "**Training loop**: We have also implemented a generic training loop for learning the diffusion model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raF5eNTLS4uc"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def train(model, optimizer, data_loader, epochs, device):\n",
        "    \"\"\"\n",
        "    Train a Diffuion model.\n",
        "\n",
        "    Parameters:\n",
        "    model: [Diffuion]\n",
        "       The model to train.\n",
        "    optimizer: [torch.optim.Optimizer]\n",
        "         The optimizer to use for training.\n",
        "    data_loader: [torch.utils.data.DataLoader]\n",
        "            The data loader to use for training.\n",
        "    epochs: [int]\n",
        "        Number of epochs to train for.\n",
        "    device: [torch.device]\n",
        "        The device to use for training.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    total_steps = len(data_loader)*epochs\n",
        "    progress_bar = tqdm(range(total_steps), desc=\"Training\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        data_iter = iter(data_loader)\n",
        "        for x in data_iter:\n",
        "            if isinstance(x, (list, tuple)):\n",
        "                x = x[0]\n",
        "            x = x.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.loss(x)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix(loss=f\"â €{loss.item():12.4f}\", epoch=f\"{epoch+1}/{epochs}\")\n",
        "            progress_bar.update()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ts-XmWqS4ue"
      },
      "source": [
        "**Training data**: Next, we load the MNIST traning set and transform the pixels to the interval $[-1,1]$. **For faster training, we initially only load the first 64 images from the traininset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKZMamhuS4uf"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# Define the transform to use for the data\n",
        "transform=transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Lambda(lambda x: x + torch.rand(x.shape)/255),\n",
        "                                transforms.Lambda(lambda x: (x-0.5)*2.0),\n",
        "                                transforms.Lambda(lambda x: x.flatten())])\n",
        "\n",
        "# Load the data\n",
        "train_data = datasets.MNIST('data/', train=True, download=True, transform=transform)\n",
        "\n",
        "# Load only the first 64 samples for training\n",
        "train_data = torch.utils.data.Subset(train_data, range(batch_size))\n",
        "\n",
        "# Create the data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Get the dimension of the dataset\n",
        "D = next(iter(train_loader))[0].shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eb0KtV1S4uh"
      },
      "source": [
        "**Initialize the model and run the training loop**: Finally we initializes the model and run the training loop. Remember that this will not work before you have completed the assignment below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QQTa8-3S4ui"
      },
      "outputs": [],
      "source": [
        "# Define the network\n",
        "from unet import Unet\n",
        "network = Unet()\n",
        "\n",
        "# Define model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\")\n",
        "model = Diffusion(network).to(device)\n",
        "\n",
        "# Define optimizer\n",
        "lr = 1e-3\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Train model\n",
        "epochs = 5000\n",
        "train(model, optimizer, train_loader, epochs, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83kjkJDKS4uj"
      },
      "source": [
        "**Sampling**: The following code samples from a trained model and plots the samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaMVYpr-S4uj"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms.functional import to_pil_image\n",
        "from torchvision.utils import make_grid\n",
        "from IPython.display import display\n",
        "\n",
        "# Generate samples\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    samples = (model.sample((64,D), T=1000)).cpu()\n",
        "    samples = (samples /2 + 0.5).clamp(0, 1)\n",
        "\n",
        "image_pil = to_pil_image(make_grid(samples.view(64, 1, 28, 28)))\n",
        "display(image_pil)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFGfH77TS4uk"
      },
      "source": [
        "# Exercise 1\n",
        "Complete the Diffusion implementation above, by implementing the following parts:\n",
        "* `Diffusion.loss(...)` should implement the denoising score matching objective. Use $\\lambda(t) = \\sigma^2_{VP}$ such that the loss just becomes $||\\tilde{s}_{\\theta}(\\mathbf{x}(t))-\\epsilon||$.\n",
        "* `Diffusion.sample(...)` should implement the SDE Euler-Maruyama solver for sampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z7YZCAlS4ul"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}