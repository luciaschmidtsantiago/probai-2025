{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e14fc24-0e0d-4513-9a71-5a374d66a539",
   "metadata": {},
   "source": [
    "## Stochastic-gradient MCMC for neural networks\n",
    "\n",
    "This notebook is for hands-on exercises during the Wednesday afternoon lecture.\n",
    "\n",
    "In the first exercise, you will implement SGD for sampling. The notebook already provides all basic definitions, including data generation and model specification, and you will only need to figure our how to set the learning rate correctly for sampling with the standard SGD.\n",
    "\n",
    "During the second break, we will work on SGLD. You will extend SGD to include additional noise and hence implement SGLD. You can also try implementing MALA, by adding Metropolis-Hasting rejection step.\n",
    "\n",
    "**DISCLAIMER**: Never use these code snippets for real problems. The notebook is purely for pedagogical reasons and the algorithms are extremely fragile and likely also buggy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc3460c-6dca-455c-b6ed-e647d018da33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import torch\n",
    "import copy\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57b8dbc-8d43-4a64-a064-e6927f98bc20",
   "metadata": {},
   "source": [
    "### Various choices\n",
    "\n",
    "These control what kind of data is generated and how many iterations we run the algorithms. Feel free to play around with these! Maybe start with linear data and model and proceed to nonlinear only after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08dba2f-a264-414c-8269-91791c74b08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set these to True to play around with a simple non-linear data\n",
    "nonlinearData = False\n",
    "nonlinearModel = False\n",
    "K = 4                    # Number of neurons in a hidden layer in the non-linear model\n",
    "\n",
    "N = 256                  # Amount of (training) data\n",
    "noiseStd = 0.2           # Noise std for data generation\n",
    "batch_size = 32          # Minibatch size (NOTE: This will be overriden in some later cells!)\n",
    "nEpoch = 1000            # Number of iterations (passes through the whole data) for optimization\n",
    "nSampling = 1000         # The same but for the sampling phase\n",
    "skip = 20                # Sampling interval -- keep only every skip:th sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f11723-4182-4679-9867-2787c1aede68",
   "metadata": {},
   "source": [
    "### Define the models\n",
    "\n",
    "Two alternative models: A simple non-linear model and a linear one that makes visualization of the posterior easy as it only has two parameters here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88071ec3-41f9-44f6-93eb-60d2c8b8cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nonlinear(nn.Module):\n",
    "  def __init__(self, D_i, D_o, K):\n",
    "      super().__init__()\n",
    "      self.fc = nn.Linear(D_i, K)\n",
    "      self.fc2 = nn.Linear(K, K)\n",
    "      self.out = nn.Linear(K, D_o)\n",
    "\n",
    "  def forward(self, x):\n",
    "      x = self.fc(x)\n",
    "      x = nn.functional.tanh(x)\n",
    "      x = self.fc2(x)\n",
    "      x = nn.functional.tanh(x)\n",
    "      x = self.out(x)\n",
    "      return x\n",
    "\n",
    "class linear(nn.Module):\n",
    "  def __init__(self, D_i, D_o):\n",
    "      super().__init__()\n",
    "      self.fc = nn.Linear(D_i, D_o)\n",
    "\n",
    "  def forward(self, x):\n",
    "      x = self.fc(x)\n",
    "      return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d159adbb-021a-4bf7-9cc3-172ebec9a322",
   "metadata": {},
   "source": [
    "### Define the objective\n",
    "\n",
    "We assume the likelihood $\\mathcal{N}(f(x), \\sigma^2)$ where $f(x)$ is the output of the prediction and the noise variance $\\sigma^2$ *is assumed to be known*. In practice this is the same things as regular mean-squared-error but we explicitly scale the objective with the noise to make sure has an interpretation as a likelihood. We could also consider $\\sigma^2$ as a random variable so that we would do Bayesian inference also over that -- think about how this would be done.\n",
    "\n",
    "The loss is written as the expected contribution for a **single data point**, $\\mathcal{L} = \\mathbb{E}[\\log p(y|x)]$. That is, the full log-likelihood would be $N \\mathcal{L}$. Writing the loss this way makes gradients and losses computed over minibatches of different size comparable and hence we do not need to change the step-length proportionally to the minibatch size, but we need to be careful in other aspects:\n",
    "1. Proper scaling of a prior is using one. The additive term needs to be $\\frac{1}{N} \\log p(\\theta)$ so that it is on the right scale\n",
    "2. When evaluating actual probabilities, for example to compute MH acceptance ratios, we need to scale the loss with $N$\n",
    "\n",
    "When reading papers about stochastic-gradient MCMC methods you need to check how they define this! Mandt et al. specifically use the per-sample losses and gradients when deriving the results for SGD, but Welling and Teh use the actual log-probability for SGLD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197acc05-9c37-4610-a460-a3e3c1222ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLLLossNormal(nn.Module):\n",
    "    def __init__(self, variance):\n",
    "        super(NLLLossNormal, self).__init__()\n",
    "        self.variance = torch.tensor(variance)\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        # Negative log-likelihood\n",
    "        log_likelihood = - 0.5 * (torch.log(2. * torch.pi * self.variance) + \n",
    "                                 (predictions - targets) ** 2 / self.variance)\n",
    "        return -torch.mean(log_likelihood)\n",
    "\n",
    "\n",
    "class NLLLossNormalSum(nn.Module):\n",
    "    def __init__(self, variance):\n",
    "        super(NLLLossNormalSum, self).__init__()\n",
    "        self.variance = torch.tensor(variance)\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        # Negative log-likelihood\n",
    "        log_likelihood = - 0.5 * (torch.log(2. * torch.pi * self.variance) + \n",
    "                                 (predictions - targets) ** 2 / self.variance)\n",
    "        return -torch.sum(log_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b2ea82-c182-4693-a9ae-320213f2751f",
   "metadata": {},
   "source": [
    "### Create data\n",
    "\n",
    "Code for creating data for simple one-dimensional regression. The inference should work also for higher-dimensional problems, but all plotting functionality assumes we have 1D inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc63847-5c03-4ddb-9575-f655492f4c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data available for training\n",
    "xobs = torch.rand(N,1)*10. - 5.\n",
    "if nonlinearData:\n",
    "    ynonoise = torch.sin(xobs/1.)\n",
    "else:\n",
    "    ynonoise = xobs/10\n",
    "yobs = ynonoise + torch.randn(N,1)*noiseStd\n",
    "\n",
    "# Test grid for plotting the full function\n",
    "G = 500\n",
    "xgrid = torch.linspace(-5.0,5.0,G).resize(G,1)\n",
    "if nonlinearData:\n",
    "    yfull = torch.sin(xgrid/1.)\n",
    "else:\n",
    "    yfull = xgrid/10.\n",
    "\n",
    "# Create mini-batches\n",
    "dataset = TensorDataset(xobs, yobs)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1b0e1f-778e-4086-9bc1-7276ea27912b",
   "metadata": {},
   "source": [
    "### Plotting functions\n",
    "\n",
    "The first plots the distribution over the functions, showing the true function, the mean estimate and all individual samples.\n",
    "\n",
    "The latter plots the posterior over the first two parameters of the model. This takes an extra argument that helps in zooming around the part of the parameter space around the MAP estimate of the linear model for ease of interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61d7710-c121-47cd-b177-937ee19cd599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_functions(x, true, data_x, data_y, pred):\n",
    "    for i in range(pred.shape[1]):\n",
    "        plt.plot(x, pred[:,i], alpha=0.1, color='r')\n",
    "    plt.plot(x, np.average(pred, axis=1), color='b')\n",
    "    plt.plot(x, true, color='k')\n",
    "    plt.plot(data_x, data_y, '.', color='k')\n",
    "    #plt.legend(['-r','-b','-k'],['Posterior samples','Mean prediction','True function'])\n",
    "\n",
    "\n",
    "def plot_posterior(weights, linearLimits=True):\n",
    "    flattened_weights = []\n",
    "    for w in weights:\n",
    "        # Stack the list of tensors into a single tensor\n",
    "        flat_tensor = torch.cat([param.flatten() for param in w.values()])\n",
    "        flattened_weights.append(flat_tensor)\n",
    "    posterior = torch.stack(flattened_weights).detach().numpy()\n",
    "\n",
    "    plt.plot(posterior[:,0], posterior[:,1], '.', color='r')\n",
    "    # Fix plotting range for the case where we know the true answer\n",
    "    if linearLimits:\n",
    "        plt.ylim([-0.3, 0.3])\n",
    "        plt.xlim([-0.3, 0.3])\n",
    "        plt.plot([0.1],[0.0],'*',color='k')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a731452-5736-413e-bf33-259a985b7406",
   "metadata": {},
   "source": [
    "## EXERCISE 1: SGD as a sampling algorithm\n",
    "\n",
    "Implement SGD as a sampling algorithm by completing the three steps:\n",
    "1. First find the optimum using standard SGD\n",
    "2. Estimate the covariance of the noise caused by using minibatches for estimating the gradient, and use it tolve for the optimal **scalar** step length (you can also try a vector-valued one if you want)\n",
    "4. Run the algorithm for enough iterations with the right step length, storing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4267fa36-8f78-4574-a99b-68b77faea217",
   "metadata": {},
   "source": [
    "### Step 1: Standard optimization\n",
    "\n",
    "Standard optimization loop that can be used for finding the best possible solution. We will be using the code as a starting point for sampling, since this is the first part of the \"SGD as sampling\" algorithm. You only need to fill in some choices for the learning hyperparameters and verify you can find the optimum.\n",
    "\n",
    "**Do not proceed** until you you have checked that you find good enough optimum. The rest of the algorithm will not be correct if we are not close to the mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33487484-eaba-4a99-96dd-a55210d533bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "if nonlinearModel:\n",
    "    model = nonlinear(1,1,K)\n",
    "else:\n",
    "    model = linear(1,1)\n",
    "\n",
    "# Optimizer\n",
    "model.train()\n",
    "objective = NLLLossNormal(variance=noiseStd**2.)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=...)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=...)\n",
    "\n",
    "losses = []\n",
    "weights = []\n",
    "for i in range(nEpoch):\n",
    "    for batch_input, batch_target in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(batch_input)\n",
    "        loss = objective(pred, batch_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Track loss using full batch data (we do not care about effiency here)\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(xobs)\n",
    "    loss = objective(pred, yobs)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # Store the parameters using deepcopy to make sure they are not references\n",
    "    # NOTE: If you want to try out the plotting functionality before implementing the sampler,\n",
    "    # you can skip from here to that cell and see how it looks like with samples stored during optimization\n",
    "    if (i + 1) % skip == 0:\n",
    "        weights.append(copy.deepcopy(model.state_dict()))\n",
    "\n",
    "# Check we optimized well enough. We know the objective for the true function and should be getting close to that.\n",
    "print(\"Final loss\", losses[-1])\n",
    "print(\"Optimal loss\", objective(ynonoise,yobs).item())\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "\n",
    "# Store the optimized model\n",
    "optimized = copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9db5dde-85e6-4b20-b3c4-f210945b881e",
   "metadata": {},
   "source": [
    "### Step 2: Estimate the covariance\n",
    "\n",
    "Implement below a piece of code that estimates the noise covariance. We will only be needing the diagonal of the covariance, so no need to estimate the whole covariance matrix.\n",
    "\n",
    "1. Loop over the minibatches and evaluate $\\mathbb{E}[(g_b-\\bar g)^2]$ where $g_b$ is the minibatch gradient and $\\bar g$ is the real gradient (that is also the average over the minibatch gradients). Re-scale this to get an estimate of the *per-sample* gradient noise, using the rule that the variance goes down linearly as a function of the minibatch size.\n",
    "2. Estimate the best scalar learning rate, and perhaps also the best vector-valued learning rate using the equatons given in the slides. Remember that trace of a diagonal matrix is the sum of the entries on the diagonal, and we are here storing only the diagonal elements.\n",
    "\n",
    "The cell already has some hints on how to manipulate the gradient objects; this is definitely not the only way but helps to get started if you are not very fluent in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df81242-7983-495a-8be9-f4ca51f4bbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Compute the mean gradient, by *not* taking a minibatch\n",
    "optimizer.zero_grad()\n",
    "pred = model(xobs)\n",
    "loss = objective(yobs, pred)\n",
    "loss.backward()\n",
    "gradmean = {name: param.grad.clone() for name, param in model.named_parameters()}\n",
    "\n",
    "# Syntax-hint: You can loop over model.named_parameters() as above and\n",
    "# if you want to store the covariance in a similar dictionary you can initialize it as\n",
    "BB = {name: torch.zeros_like(param) for name, param in model.named_parameters()} \n",
    "\n",
    "# Compute the diagonal covariance over the minibatches\n",
    "for batch_input, batch_target in data_loader:\n",
    "    # Compute the mini-batch gradient\n",
    "    ...\n",
    "    \n",
    "    # Store the second powers\n",
    "    ...\n",
    "\n",
    "# Compute the trace (by summing over the different parameters)\n",
    "# Remember to also scale the covariances right\n",
    "\n",
    "TraceBB = ...\n",
    "D = ...\n",
    "\n",
    "# Once you have the following quantities you can compute the optimal learning rate\n",
    "# - D is the number of parameters\n",
    "# - BB is a dictionary similar to the gradient, but having the per-sample diagonal covariance\n",
    "# - TraceBB is the trace of that\n",
    "\n",
    "print(\"Observations: \", N)\n",
    "print(\"Dimensionality: \", D)\n",
    "print(\"Batch size: \", batch_size)\n",
    "print(\"Trace of noise covariance: \",TraceBB)\n",
    "\n",
    "eps = ...\n",
    "print(\"Optimal scalar learning rate: \",eps)\n",
    "\n",
    "eps_vector = {name: ... for name, param in model.named_parameters()}\n",
    "print(\"Optimal vector-valued learning rate:\", eps_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf465bb6-3846-4232-aecc-5f1397a7b15d",
   "metadata": {},
   "source": [
    "#### Step 3: Sample using the right learning rate\n",
    "\n",
    "Now run the actual sampler. This goes exactly like in the initial optimization phase, but now we should:\n",
    "1. Start from the already trained solution, to guarantee we are around the MAP estiamte\n",
    "2. Use a constant learning rate -- no scheduler!\n",
    "3. Store samples every now and then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aadf8eb-09f0-4c5d-9ddc-ee4baf532f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the model we trained\n",
    "model.load_state_dict(optimized)\n",
    "\n",
    "# Scalar or vector-valued learning rate\n",
    "vector_valued = False\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=eps)\n",
    "\n",
    "weights = []\n",
    "for i in range(nSampling):\n",
    "    for batch_input, batch_target in data_loader:\n",
    "        # Compute the gradient\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(batch_input)\n",
    "        loss = objective(pred, batch_target)\n",
    "        loss.backward()\n",
    "\n",
    "        # Vector-valued optimization implemented manually\n",
    "        if vector_valued:\n",
    "            with torch.no_grad():\n",
    "                for name, param in model.named_parameters():\n",
    "                    param -= ...\n",
    "        else:\n",
    "            optimizer.step()\n",
    "\n",
    "    # Store the parameters using deepcopy to make sure they are not references\n",
    "    if (i + 1) % skip == 0:\n",
    "        weights.append(copy.deepcopy(model.state_dict()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb97975-97c6-43d1-9264-0c1a516ea020",
   "metadata": {},
   "source": [
    "### Evaluation: Make predictions with all models and plot the results\n",
    "\n",
    "Let's evaluate how the model works. Once you can see it does something reasonable, try playing around with:\n",
    "1. N and the batch size\n",
    "2. Linear and non-linear data\n",
    "\n",
    "The code below already has plotting functionality that should work for any collection of models stored 'weights'. The first plot shows the functions, the latter the posterior over the first two parameters of the model. For the linear model it already zooms to the right area of the parameter space for the specific linear data used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4a0f9b-54ca-444a-9b63-4b4d0a8c5919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with all stored weights\n",
    "preds = []\n",
    "for i in range(len(weights)):\n",
    "    model.load_state_dict(weights[i])\n",
    "    model.eval()\n",
    "    preds.append(model(xgrid))\n",
    "pred = torch.cat(preds,dim=1)\n",
    "\n",
    "plot_functions(xgrid.detach().numpy(), yfull.detach().numpy(), xobs.detach().numpy(), yobs.detach().numpy(), pred.detach().numpy())\n",
    "plt.show()\n",
    "\n",
    "plot_posterior(weights,not nonlinearModel)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecdc375-911a-47ab-9bd3-45d310d85cfa",
   "metadata": {},
   "source": [
    "### Exercise 1 end here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f54298-5a39-4219-98dc-f453ee6c6589",
   "metadata": {},
   "source": [
    "## EXERCISE 2: Stochastic gradient Langevin dynamics\n",
    "\n",
    "Implement SGLD:\n",
    "1. Write a standard SGD optimization loop, with suitable learning rate scheduling\n",
    "2. Add explicit Gaussian noise after making the gradient step (or replace the gradient step with a manual update)\n",
    "3. Store samples as before and average predictions over the posterior. Run the sampler for nEpochs + nSampling and only store samples during the latter stage.\n",
    "\n",
    "Note that now we should be using the actual log-probability as the loss, not the expectation for a single sample. Think about how this influences the optimal learning rate.\n",
    "\n",
    "Track somehow the learning rates during the algorithm and try to evaluate whether we are in the SGLD sampling regime. According to the theory, we should be having the injective noise dominating the gradient noise, which we could try to validate using:\n",
    "- The gradient noise is proportional to $\\epsilon^2$ but also $BB^T$\n",
    "- The injected noise variance is $2 \\epsilon$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79411f8a-3484-41a4-9210-ce4897472b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "if nonlinearModel:\n",
    "    model = nonlinear(1,1,K)\n",
    "else:\n",
    "    model = linear(1,1)\n",
    "\n",
    "model.train()\n",
    "objective = NLLLossNormalSum(variance=noiseStd**2.)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=...)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=...)\n",
    "\n",
    "print(\"Initial learning rate: \", scheduler.get_last_lr()[0])\n",
    "\n",
    "# Sampling loop\n",
    "losses = []\n",
    "weights = []\n",
    "nEpoch = 2000\n",
    "for i in range(nEpoch + nSampling):\n",
    "    for batch_input, batch_target in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(batch_input)\n",
    "        loss = objective(pred, batch_target)\n",
    "        loss.backward()\n",
    "\n",
    "        # Replace standard gradient update with a manual one where we make a gradient step and add noise as well\n",
    "        # Since we use lr directly as the learning rate, we need to multiply the noise scale with sqrt(2)\n",
    "        # The code already has a hint on how to replace the weights of the network with the proposal\n",
    "        scale = ...\n",
    "        with torch.no_grad():\n",
    "            proposal = ...\n",
    "            model.load_state_dict({name: proposal[name] for name in proposal})\n",
    "\n",
    "    # Decrease learning rate\n",
    "    # Note that this will give a warning as we never call optimizer.step(), but it still works correctly\n",
    "    scheduler.step()\n",
    "\n",
    "    # Track loss using full batch data (we do not care about effiency here)\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(xobs)\n",
    "    loss = objective(pred, yobs)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if (i == nEpoch):\n",
    "        print(\"Learning rate when entering sampling: \", scheduler.get_last_lr()[0])\n",
    "    \n",
    "    if (i > nEpoch) and ((i + 1) % skip == 0):\n",
    "        weights.append(copy.deepcopy(model.state_dict()))\n",
    "\n",
    "print(\"Final learning rate: \", scheduler.get_last_lr()[0])\n",
    "\n",
    "# Check we optimized well enough. We know the objective for the true function and should be getting close to that.\n",
    "print(\"Final loss\", losses[-1])\n",
    "print(\"Optimal loss\", objective(ynonoise,yobs).item())\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4a6fde-16cd-40c0-9bfa-8f41daaa3bc5",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Look at the same things as above. Again try to play around with $N$ and batch size, but also with the initial learning rate and the scheduling. You might find the results quite sensitive to the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cc662d-29ce-4d4b-8d0e-df2e47e09db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for i in range(len(weights)):\n",
    "    model.load_state_dict(weights[i])\n",
    "    model.eval()\n",
    "    preds.append(model(xgrid))\n",
    "pred = torch.cat(preds,dim=1)\n",
    "\n",
    "plot_functions(xgrid.detach().numpy(), yfull.detach().numpy(), xobs.detach().numpy(), yobs.detach().numpy(), pred.detach().numpy())\n",
    "#plt.savefig(\"SGLD-new-N512.png\")\n",
    "plt.show()\n",
    "\n",
    "plot_posterior(weights, not nonlinearModel)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd03e91-5c81-4552-9ada-57976635b790",
   "metadata": {},
   "source": [
    "### Exercise 2 (BONUS): Metropolis-adjusted Langevin algorithm (MALA)\n",
    "\n",
    "If you have time after completing the SGLD, feel free to attempt implementing the full MALA algorithm.\n",
    "\n",
    "1. Proceed using the same basic loop, but now instead of optimizet.step() you need to *propose* a new configuration while storing still the old one. Compute the likelihood for the new configuration.\n",
    "2. Form explicitly a normal distribution with the centroid at the gradient step and the covariance matching the additive noise, and compute the proposal distribution q().\n",
    "3. Do the same to the opposite direction! Assume we are at the newly proposed configuration, evaluate gradients etc from there and again form the proposal distribution as well.\n",
    "4. Finally, use MH to select whether we keep the new proposal or not.\n",
    "\n",
    "Do not worry about computational efficiency at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d23264-6db1-4870-9e30-d45d47272f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the model we trained, just to avoid needig to find the mode again\n",
    "#model.load_state_dict(optimized)\n",
    "# Initialize the model\n",
    "if nonlinearModel:\n",
    "    model = nonlinear(1,1,K)\n",
    "else:\n",
    "    model = linear(1,1)\n",
    "\n",
    "# Set this to False to run SGLD with this code\n",
    "MHcheck = True\n",
    "\n",
    "model.train()\n",
    "objective = NLLLossNormalSum(variance=noiseStd**2.)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=...)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=...)\n",
    "\n",
    "print(\"Initial learning rate: \", scheduler.get_last_lr()[0])\n",
    "\n",
    "# Sampling loop\n",
    "losses = []\n",
    "weights = []\n",
    "accepted = []\n",
    "total = 0\n",
    "nEpoch = 2000\n",
    "for i in range(nEpoch + nSampling):\n",
    "    for batch_input, batch_target in data_loader:\n",
    "        # Store current weights, we need them for evaluating the acceptance and may need to return to that state\n",
    "        current = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        # Find the current gradient\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(batch_input)\n",
    "        loss = objective(pred, batch_target)\n",
    "        loss.backward()\n",
    "\n",
    "        # Make a proposal distribution, a Gaussian with:\n",
    "        # - Centroid is current parameters - gradient with the right step length\n",
    "        # - Covariance is diagonal with the right variance\n",
    "        # - The actual proposal is a sample from that distribution\n",
    "        # - This part is essentially the same as in SGLD, just storing also the centroid explicitly for future use\n",
    "        mu = ...\n",
    "        with torch.no_grad():\n",
    "            proposal = ...\n",
    "            model.load_state_dict({name: proposal[name] for name in proposal})\n",
    "\n",
    "        \n",
    "        # The Metropolis-Hastings step\n",
    "        if MHcheck:\n",
    "            # Evaluate the log-likelihood and the proposal q()\n",
    "            new_log_prob = ...\n",
    "            q_old_to_new = ...\n",
    "    \n",
    "            # Form the opposite proposal:\n",
    "            # Gradient from that point\n",
    "            ...\n",
    "            # Centroid, log-probability and proposal q()\n",
    "            # Note that you to swith back to the old state to evaluate its probability\n",
    "            mu2 = ...\n",
    "            ...\n",
    "            old_log_prob = ...\n",
    "            q_new_to_old = ...\n",
    "    \n",
    "            # Perform the MH acceptance check and track acceptance ratio\n",
    "            ratio = torch.exp(...)\n",
    "            if torch.rand(1) < ratio:\n",
    "                model.load_state_dict({name: proposal[name] for name in proposal})\n",
    "                accepted.append(1)\n",
    "            else:\n",
    "                model.load_state_dict({name: current[name] for name in current})\n",
    "                accepted.append(0)\n",
    "            total += 1\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Track loss using full batch data (we do not care about effiency here)\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(xobs)\n",
    "    loss = objective(pred, yobs)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if (i == nEpoch):\n",
    "        print(\"Learning rate when entering sampling: \", scheduler.get_last_lr()[0])\n",
    "\n",
    "    if (i > nEpoch) and ((i + 1) % skip == 0):\n",
    "        weights.append(copy.deepcopy(model.state_dict()))\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "if MHcheck:\n",
    "    plt.plot(np.cumsum(accepted)/np.cumsum(np.ones(len(accepted))))\n",
    "    plt.show()\n",
    "    print(\"Acceptance ratio:\", sum(accepted)/total)\n",
    "\n",
    "print(\"Final learning rate\", lr)\n",
    "print(\"Final loss\", losses[-1])\n",
    "print(\"Optimal loss\", objective(ynonoise,yobs).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a76c58-77d1-41f0-9a15-69c48371fa1e",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Look at the same things as above. Again try to play around with $N$ and batch size, but also with the initial learning rate and the scheduling. You might find the results quite sensitive to the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224cb2bb-9622-4650-8bf8-421ff8bc88fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for i in range(len(weights)):\n",
    "    model.load_state_dict(weights[i])\n",
    "    model.eval()\n",
    "    preds.append(model(xgrid))\n",
    "pred = torch.cat(preds,dim=1)\n",
    "\n",
    "plot_functions(xgrid.detach().numpy(), yfull.detach().numpy(), xobs.detach().numpy(), yobs.detach().numpy(), pred.detach().numpy())\n",
    "plt.show()\n",
    "\n",
    "plot_posterior(weights, not nonlinearModel)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
